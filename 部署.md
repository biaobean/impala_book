# 配置Impala

## 更正路径

```sh
export BUILD_TYPE=debug

cd $IMPALA_HOME/

rm be/build/latest
ln -s $BUILD_TYPE latest

rm be/build/$BUILD_TYPE/catalog/catalogd
ln -s ../service/impalad be/build/$BUILD_TYPE/catalog/catalogd

rm be/build/$BUILD_TYPE/statestore/statestored
ln -s ../service/impalad be/build/$BUILD_TYPE/statestore/statestored
```

# 打包Impala

```sh
## 函数，将原来$IMPALA_HOME制定目录下所有文件及子目录拷贝到当前目录，并保持相对路径不变
function copy_dir() {
  if [[ "$1" = "" ]]; then
    return
  fi

  mkdir -p $1
  cp -r $IMPALA_HOME/$1/* $1
}

function copy_dir_in_paths() {
  if [[ "$1" = "" ]]; then
    return
  fi

  IFS=: DIRS=($1)
  declare -p DIRS
  for f in "${DIRS[@]}"; do
    if [[ "$f" = "$IMPALA_HOME" ]]; then
      continue
    fi
    REDIR=${f#${IMPALA_HOME}/}
    [[ ! -e ${REDIR} ]] || continue
    copy_dir $REDIR
  done
  IFS=
}

source $IMPALA_HOME/bin/impala-config.sh

copy_dir bin
copy_dir www ## Web UI
copy_dir infra
copy_dir be/build
copy_dir fe/src/test/resources
copy_dir testdata/common
copy_dir testdata/cluster ## 集群启动脚本

## 将Impala目录下所有.so结尾的文件都复制到lib/目录下
mkdir lib
find $IMPALA_HOME -name '*.so' | xargs -i cp {} ./lib

## 复制Java文件
mkdir -p fe/target
cp -r $IMPALA_HOME/fe/target/dependency fe/target
cp -r $IMPALA_HOME/fe/target/classes fe/target
cp $IMPALA_HOME/fe/target/*.jar fe/target

## 复制依赖包
cp ${HADOOP_LIB_DIR}/native/*.so ./lib
cp ${IMPALA_SNAPPY_PATH}/*.so ./lib
cp ${IMPALA_LZO}/build/*.so ./lib

export USE_SYSTEM_GCC=${USE_SYSTEM_GCC-0}
if [ $USE_SYSTEM_GCC -eq 0 ]; then
  cp ${IMPALA_TOOLCHAIN_GCC_LIB}/*.so ./lib
fi

## 加入Python依赖
copy_dir_in_paths $PYTHONPATH


## 添加路径
echo '
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$IMPALA_HOME/lib
echo Added lib/ dir, now LD_LIBRARY_PATH=$LD_LIBRARY_PATH
' >> bin/impala-config.sh
```

**\*\*\*该部分操作使用hadoop用户操作**

## 2.1更改集群配置文件

1. 删除$IMPALA\_FE\_DIR/src/test/resources目录下的文件，并将集群的配置文件到此目录，至少包括hdfs-site.xml、core-site.xml和hive-site.xml等文件。（这是impala-config.sh文件中设置的Hadoop配置文件目录，可以修改指向其他位置）

2. 在hdfs-site.xml中添加

```xml
<property>
  <name>dfs.client.read.shortcircuit</name>
  <value>true</value>
</property>
<property>
  <name>dfs.domain.socket.path</name>
  <value>/var/lib/hadoop-hdfs/dn_socket</value>
</property>
<property>
  <name>dfs.datanode.data.dir.perm</name>
  <value>755</value>
</property>
<property>
  <name>dfs.block.local-path-access.user</name>
  <value>hadoop</value>
</property>
<property>
  <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
  <value>true</value>
</property>
<property>
  <name>dfs.client.file-block-storage-locations.timeout</name>
  <value>10000</value>
</property>
```

其中dfs.client.read.shortcircuit和 dfs.domain.socket.path为hdfs的短读属性，详细信息参见[http://note.youdao.com/share/web/file.html?id=fdc808678a8a9362fc3f8493babea5bc&type=note](http://note.youdao.com/share/web/file.html?id=fdc808678a8a9362fc3f8493babea5bc&type=note)

2.在core-site.xml中添加

```xml
<property>
  <name>dfs.client.read.shortcircuit</name>
  <value>true</value>
</property>
<property>
  <name>dfs.client.read.shortcircuit.skip.checksum</name>
  <value>false</value>
</property>
```


3.注释掉

\#for jar in \`ls ${IMPALA\_HOME}/testdata/target/dependency/\*.jar\`; do

\# CLASSPATH=${CLASSPATH}:$jar

\#done

这三行

## 2.6在start-statestore 中

vi bin/start-statestore.sh

1. 添加so动态库路径以及nohup功能

![](file://localhost/Users/biaochen/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image002.png)2.在start-impalad.sh、start-catalogd添加nohup功能

![](file://localhost/Users/biaochen/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image004.png)

![](file://localhost/Users/biaochen/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image006.png)

## 2.7添加mysql驱动

将hive下的

hive/lib/mysql-connector-java-5.1.30.jar

拷贝到

impala /fe/target/dependency/下

cp /opt/beh/core/hive/lib/mysql-connector-java-5.1.30.jar impala /fe/target/dependency/



# 三、将impala部署到集群其他机器上

## 3.1转移boost

1.切换到 root 用户下

su root

2.将本地boost库打包

find /usr/local -name '\*boost\*'

tar -czvf boostok.tgz /usr/local/lib/libboost\_\*

3.将boost包分发到其他节点上

scp boostok.tgz root@hadoop002:/opt/

scp boostok.tgz root@hadoop003:/opt/

scp boostok.tgz root@hadoop004:/opt/

4.到其他机器上解压

ssh hadoop002

cd /opt

tar -xzvf boostok.tgz -C /

hadoop003和hadoop004同上操作

5.查看一下两边boost包的数量是否一样

find /usr/local/lib -name 'libboost\*' \|wc -l

6.删除压缩包

rm boostok.tgz

7.切回hadoop用户

su hadoop

## 3.2转移llvm

1.切换到root用户

su root

2.将本地llvm打包

find /usr -name '\*llvm\*

tar -czvf llvmok.tgz /usr/local/bin/llvm\*

3.将llvm包分发到其他节点上

scp llvmok.tgz root@hadoop002:/opt/

scp llvmok.tgz root@hadoop003:/opt/

scp llvmok.tgz root@hadoop004:/opt/

4.到其他机器上解压

ssh hadoop002

cd /opt

tar -xzvf llvmok1.tgz -C /

hadoop003和hadoop004操作同上

5.删除压缩包

rm llvmok.tgz

6.切回hadoop用户

su hadoop

## 3.3转移impala

本操作使用hadoop用户

1.将本地impala打包

cd /opt/beh/core/

tar -jcvf impala-release.tar.bz2 impala

2.分发到其他节点上

scp impala-release.tar.bz2 hadoop@hadoop002:/opt/beh/core/

scp impala-release.tar.bz2 hadoop@hadoop003:/opt/beh/core/

scp impala-release.tar.bz2 hadoop@hadoop004:/opt/beh/core/

3.登录到其他节点上解压

ssh hadoop002

tar -jxvf impala-release.tar.bz2

4.删除压缩包

rm impala-release.tar.bz2

5.配置此节点环境变量

vi /opt/beh/cobf/beh\_env

1\)添加

export IMPALA\_HOME=$BEH\_HOME/core/impala

export IMPALA\_CONF\_DIR=$IMPALA\_HOME/conf

2\) source /opt/beh/cobf/beh\_env

# 四、启动impala

## 4.1需要进程

impala需要启动的进程：

statestore

catalog

impalad

其中statestore和catalog只需在其中一台节点启动即可，其他节点只需启动impalad进程

## 4.2启动命令

在impala目录下执行

cd /opt/beh/core/impala

启动statestore命令

./bin/start-statestored.sh

启动catalog命令

./bin/start-catalogd.sh

启动impalad命令

./bin/start-impalad.sh

## 4.3在要进行查询的节点上启动impala-shell

进入impala目录

./bin/impala-shell.sh

```shell
buildall.sh -release -noclean -skiptests -so
```

## 修改符号链接

## 配置

${IMPALA\_HOME}/fe/src/test/resources

hdfs-site.xml

flag file

`-beeswax_port=21000`

`-fe_port=21000`

`-be_port=22000`

`-llama_callback_port=28000`

`-hs2_port=21050`

`-enable_webserver=true`

`-mem_limit=3803185152`

`-max_log_files=10`

`-webserver_port=25000`

`-max_result_cache_size=100000`

`-state_store_subscriber_port=23000`

`-statestore_subscriber_timeout_seconds=30`

`-scratch_dirs=/impala/impalad`

`-default_query_options`

`-load_auth_to_local_rules=false`

`-log_filename=impalad`

`-audit_event_log_dir=/var/log/impalad/audit`

`-max_audit_event_log_file_size=5000`

`-abort_on_failed_audit_event=false`

`-minidump_path=/var/log/impala-minidumps`

`-max_minidumps=9`

`-lineage_event_log_dir=/var/log/impalad/lineage`

`-max_lineage_log_file_size=5000`

`-hostname=ip-172-31-20-161.ap-northeast-2.compute.internal`

`-state_store_host=ip-172-31-20-161.ap-northeast-2.compute.internal`

`-enable_rm=false`

`-state_store_port=24000`

`-catalog_service_host=ip-172-31-20-161.ap-northeast-2.compute.internal`

`-catalog_service_port=26000`

`-local_library_dir=/var/lib/impala/udfs`

`-fair_scheduler_allocation_path=/var/run/cloudera-scm-agent/process/51-impala-IMPALAD/impala-conf/fair-scheduler.xml`

`-llama_site_path=/var/run/cloudera-scm-agent/process/51-impala-IMPALAD/impala-conf/llama-site.xml`

`-disable_admission_control=false`

`-queue_wait_timeout_ms=60000`

`-disk_spill_encryption=false`

`-abort_on_config_error=true`

