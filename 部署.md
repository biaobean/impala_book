# 配置Impala

## 更正路径

```
export BUILD_TYPE=debug

cd $IMPALA_HOME/

rm be/build/latest
ln -s $BUILD_TYPE latest

rm be/build/$BUILD_TYPE/catalog/catalogd
ln -s ../service/impalad be/build/$BUILD_TYPE/catalog/catalogd

rm be/build/$BUILD_TYPE/statestore/statestored
ln -s ../service/impalad be/build/$BUILD_TYPE/statestore/statestored
```


# 打包Impala


```
PACKAGE_ROOT_DIR=`pwd`
source $IMPALA_HOME/bin/impala-config.sh
## export IMPALA_HOME=$PACKAGE_ROOT_DIR

cp -r $IMPALA_HOME/bin .
mkdir be
cp -r $IMPALA_HOME/be/build be

mkdir -p fe/src/test
cp -r $IMPALA_HOME/fe/src/test/resources fe/src/test

mkdir lib

## 2.将impala当前目录下所有.so结尾的文件都复制到so目录下

find $IMPALA_HOME -name '*.so' | xargs -i cp {} ./lib

## Java
mkdir -p fe/target
cp -r $IMPALA_HOME/fe/target/dependency fe/target
cp -r $IMPALA_HOME/fe/target/classes fe/target
cp $IMPALA_HOME/fe/target/*.jar fe/target



```


**\*\*\*该部分操作使用hadoop用户操作**

## 2.1更改集群配置文件

1. 删除$IMPALA_FE_DIR/src/test/resources目录下的文件，并将集群的配置文件到此目录，包括hdfs-site.xml、core-site.xml等文件。（这是impala-config.sh文件中设置的Hadoop配置文件目录，可以修改指向其他位置）

2. 在hdfs-site.xml中添加

&lt;property&gt;

&lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;

&lt;value&gt;true&lt;/value&gt;

&lt;/property&gt;

&lt;property&gt;

&lt;name&gt;dfs.domain.socket.path&lt;/name&gt;

&lt;value&gt;/var/lib/hadoop-hdfs/dn\_socket&lt;/value&gt;

&lt;/property&gt;

&lt;property&gt;

&lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;

&lt;value&gt;755&lt;/value&gt;

&lt;/property&gt;

&lt;property&gt;

&lt;name&gt;dfs.block.local-path-access.user&lt;/name&gt;

&lt;value&gt;hadoop&lt;/value&gt;

&lt;/property&gt;

&lt;property&gt;

&lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;

&lt;value&gt;true&lt;/value&gt;

&lt;/property&gt;

&lt;property&gt;

&lt;name&gt;dfs.client.file-block-storage-locations.timeout&lt;/name&gt;

&lt;value&gt;10000&lt;/value&gt;

&lt;/property&gt;

其中dfs.client.read.shortcircuit和 dfs.domain.socket.path为hdfs的短读属性，详细信息参见

[http://note.youdao.com/share/web/file.html?id=fdc808678a8a9362fc3f8493babea5bc&type=note](http://note.youdao.com/share/web/file.html?id=fdc808678a8a9362fc3f8493babea5bc&type=note)

2.在core-site.xml中添加

&lt;property&gt;

&lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;

&lt;value&gt;true&lt;/value&gt;

&lt;/property&gt;

&lt;property&gt;

&lt;name&gt;dfs.client.read.shortcircuit.skip.checksum&lt;/name&gt;

&lt;value&gt;false&lt;/value&gt;

&lt;/property&gt;

## 2.2删除impala自带配置文件样例

**使用hadoop用户操作**

**进入impala目录，删除impala自带的site.xml文件样例**

cd impala

find . -name core-site.xml

find . -name hdfs-site.xml

find . -name hive-site.xml

find . -name yarn-site.xml

find之后将这些配置文件删除

## 2.3拷贝配置文件

1.在impala目录下创建confmulu

cd /opt/beh/core/impala

mkdir conf

2.将之前修改过的本地集群上的配置文件core-site.xml

hdfs-site.xml以及hive-site.xml拷贝至conf目录下

cp /opt/beh/core/hadoop/etc/hadoop/core-site.xml conf/

cp /opt/beh/core/hadoop/etc/hadoop/hdfs-site.xml conf/

cp /opt/beh/core/hive/conf/hive-site.xml conf/

## 2.4建立so动态库目录

1.在impala目录下新建so目录

mkdir so

2.将impala当前目录下所有.so结尾的文件都复制到so目录下

find . -name '\*.so' \| xargs -i cp {} ./so

**注：在改命令在/opt/beh/core/impala/下执行**

3.将impala目录下的tests、testdata、thirtyparty三个目录删除

rm -rf test\*

rm -rf thirtyparty

## 2.5修改set-classpath.sh

vi bin/set-classpath.sh

1.在CLASSPATH中添加

$IMPALA\_HOME/conf:\

2.添加so动态库目录路径

export LD\_LIBRARY\_PATH=$LD\_LIBRARY\_PATH:$IMPALA\_HOME/so/

3.注释掉

\#for jar in \`ls ${IMPALA\_HOME}/testdata/target/dependency/\*.jar\`; do

\# CLASSPATH=${CLASSPATH}:$jar

\#done

这三行

## 2.6在start-statestore 中

vi bin/start-statestore.sh

1. 添加so动态库路径以及nohup功能

![](file://localhost/Users/biaochen/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image002.png)2.在start-impalad.sh、start-catalogd添加nohup功能

![](file://localhost/Users/biaochen/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image004.png)

![](file://localhost/Users/biaochen/Library/Group%20Containers/UBF8T346G9.Office/msoclip1/01/clip_image006.png)

## 2.7添加mysql驱动

将hive下的

hive/lib/mysql-connector-java-5.1.30.jar

拷贝到

impala /fe/target/dependency/下

cp /opt/beh/core/hive/lib/mysql-connector-java-5.1.30.jar impala /fe/target/dependency/

## 2.8添加impala环境变量

vi /opt/beh/conf/beh\_env

1.添加

export IMPALA\_HOME=$BEH\_HOME/core/impala

export IMPALA\_CONF\_DIR=$IMPALA\_HOME/conf

2.source /opt/beh/conf/beh\_env

# 三、将impala部署到集群其他机器上

## 3.1转移boost

1.切换到 root 用户下

su root

2.将本地boost库打包

find /usr/local -name '\*boost\*'

tar -czvf boostok.tgz /usr/local/lib/libboost\_\*

3.将boost包分发到其他节点上

scp boostok.tgz root@hadoop002:/opt/

scp boostok.tgz root@hadoop003:/opt/

scp boostok.tgz root@hadoop004:/opt/

4.到其他机器上解压

ssh hadoop002

cd /opt

tar -xzvf boostok.tgz -C /

hadoop003和hadoop004同上操作

5.查看一下两边boost包的数量是否一样

find /usr/local/lib -name 'libboost\*' \|wc -l

6.删除压缩包

rm boostok.tgz

7.切回hadoop用户

su hadoop

## 3.2转移llvm

1.切换到root用户

su root

2.将本地llvm打包

find /usr -name '\*llvm\*

tar -czvf llvmok.tgz /usr/local/bin/llvm\*

3.将llvm包分发到其他节点上

scp llvmok.tgz root@hadoop002:/opt/

scp llvmok.tgz root@hadoop003:/opt/

scp llvmok.tgz root@hadoop004:/opt/

4.到其他机器上解压

ssh hadoop002

cd /opt

tar -xzvf llvmok1.tgz -C /

hadoop003和hadoop004操作同上

5.删除压缩包

rm llvmok.tgz

6.切回hadoop用户

su hadoop

## 3.3转移impala

本操作使用hadoop用户

1.将本地impala打包

cd /opt/beh/core/

tar -jcvf impala-release.tar.bz2 impala

2.分发到其他节点上

scp impala-release.tar.bz2 hadoop@hadoop002:/opt/beh/core/

scp impala-release.tar.bz2 hadoop@hadoop003:/opt/beh/core/

scp impala-release.tar.bz2 hadoop@hadoop004:/opt/beh/core/

3.登录到其他节点上解压

ssh hadoop002

tar -jxvf impala-release.tar.bz2

4.删除压缩包

rm impala-release.tar.bz2

5.配置此节点环境变量

vi /opt/beh/cobf/beh\_env

1\)添加

export IMPALA\_HOME=$BEH\_HOME/core/impala

export IMPALA\_CONF\_DIR=$IMPALA\_HOME/conf

2\) source /opt/beh/cobf/beh\_env

# 四、启动impala

## 4.1需要进程

impala需要启动的进程：

statestore

catalog

impalad

其中statestore和catalog只需在其中一台节点启动即可，其他节点只需启动impalad进程

## 4.2启动命令

在impala目录下执行

cd /opt/beh/core/impala

启动statestore命令

./bin/start-statestored.sh

启动catalog命令

./bin/start-catalogd.sh

启动impalad命令

./bin/start-impalad.sh

## 4.3在要进行查询的节点上启动impala-shell

进入impala目录

./bin/impala-shell.sh

```shell
buildall.sh -release -noclean -skiptests -so
```

## 修改符号链接

## 配置

${IMPALA\_HOME}/fe/src/test/resources

hdfs-site.xml

flag file

`-beeswax_port=21000`

`-fe_port=21000`

`-be_port=22000`

`-llama_callback_port=28000`

`-hs2_port=21050`

`-enable_webserver=true`

`-mem_limit=3803185152`

`-max_log_files=10`

`-webserver_port=25000`

`-max_result_cache_size=100000`

`-state_store_subscriber_port=23000`

`-statestore_subscriber_timeout_seconds=30`

`-scratch_dirs=/impala/impalad`

`-default_query_options`

`-load_auth_to_local_rules=false`

`-log_filename=impalad`

`-audit_event_log_dir=/var/log/impalad/audit`

`-max_audit_event_log_file_size=5000`

`-abort_on_failed_audit_event=false`

`-minidump_path=/var/log/impala-minidumps`

`-max_minidumps=9`

`-lineage_event_log_dir=/var/log/impalad/lineage`

`-max_lineage_log_file_size=5000`

`-hostname=ip-172-31-20-161.ap-northeast-2.compute.internal`

`-state_store_host=ip-172-31-20-161.ap-northeast-2.compute.internal`

`-enable_rm=false`

`-state_store_port=24000`

`-catalog_service_host=ip-172-31-20-161.ap-northeast-2.compute.internal`

`-catalog_service_port=26000`

`-local_library_dir=/var/lib/impala/udfs`

`-fair_scheduler_allocation_path=/var/run/cloudera-scm-agent/process/51-impala-IMPALAD/impala-conf/fair-scheduler.xml`

`-llama_site_path=/var/run/cloudera-scm-agent/process/51-impala-IMPALAD/impala-conf/llama-site.xml`

`-disable_admission_control=false`

`-queue_wait_timeout_ms=60000`

`-disk_spill_encryption=false`

`-abort_on_config_error=true`

